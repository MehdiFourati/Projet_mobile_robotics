{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report project Basics of mobile robotics\n",
    "Members: Mahdi Fourati, Yahya Hadi, Fatih Yilmaz, Alex Zanetta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook serves as a final report for the project of Basics of mobile robotics. We will first go over our initial choices, then make a deep dive into the specific parts of the code.\n",
    "\n",
    "### Initial choices\n",
    "* Environment: we decided to use a simple blue floor with black obstacles and a red objective to have a strong contrast between them and the robot to make the vision easier. The obstacles can have any shape, which make the math heavier but more representative of the reality.\n",
    "* Global navigation: the path from the starting point to the goal is computed using Dijkstra's algorithm. We decided to use Dijkstra's algorithm to allow us more flexibility in case we wanted to further complexify the problem and add some intermediary goals (which doesn't work with heuristics based algorithm such as A*). The points that the algorithm uses are the vertices of the obstacles approximated a bit more than half the robot's width away to avoid crashing into them while taking the uncertainty into account. \n",
    "* Pose estimation : the filter we used to estimate the pose and to control the robot is an extended Kalman filter. We chose it because all the prerequisite for its use where fullfilled and it is the de facto standard in navigation systems [[1]](#1).\n",
    "* Motion control: we used a standard P-lead controller to make the robot follow the trajectory. As the trajectory is linear and the odometry reasonably good (the same speed on the two wheels will make the robot go straight with very little error), we don't need to make it more complex and use a PID controller or any othere more complex.\n",
    "* Local navigation: this part makes the robot avoid crashing into obstacles if it senses something right in front of him. The shape of the obstacles is not relevant: the robot will move around it in the direction where it didn't sense anything. If all the sensors sense something, i.e. the robot is in a corner, it will do a 180°.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Vision\n",
    "\n",
    "### Getting the field of play\n",
    "\n",
    "To get the field of play (FoP), we begin by getting the four corners of the blue rectangle on the raw video feed, and approximate it by a four corners polygon. In order to find the corners, we threshold the input image on green and red, look for the intersection between and invert it. The operation of threshholding takes an image with a single colour channel (by opposition to the standard 3 colours RGB format), and sets all pixel below a threshold parameter to 0 (black) and all above to 255 (white):\n",
    "\n",
    "$$\n",
    "out(x,y)=\n",
    "\\begin{cases}\n",
    "255 & \\quad \\text{when $src(x,y) > parameter$}\\\\ \n",
    "0 & \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We use a thresh parameter low enough to get the whole border but high enough to avoid getting the FoP. Once we have done the thresholding on the two colour channel green and red we make a bitwise AND on them:\n",
    "\n",
    "$$\n",
    "out(x,y)=\n",
    "\\begin{cases}\n",
    "255 & \\quad \\text{when $thresh_{green}(x,y) \\neq 0 \\: \\& \\: thresh_{red}(x,y) \\neq 0$}\\\\ \n",
    "0 & \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally, we invert the result. This yields the FoP as a white shape on a black background. This allows us to find the contours of the shape. They are all the boundary points of a white object on a black background. We make the computations more efficient by throwing away all unnecessary points (e.g. we don't need all points of a straight line, just the first and the last). This is the code doing this part:  \n",
    "\n",
    "---\n",
    "```r\n",
    "_, thresholded_green = cv.threshold(copy[:,:,1],127,255,cv.THRESH_BINARY)\n",
    "_, thresholded_red = cv.threshold(copy[:,:,2],127,255,cv.THRESH_BINARY)\n",
    "\n",
    "thresholded = thresholded_green & thresholded_red\n",
    "thresholded = (255 - thresholded)\n",
    "\n",
    "contours, _ = cv.findContours(thresholded, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We chose to use a blue floor to make the contrasts as large as possible with the other elements. Since the camera is not perfectly over the FoP, the raw video feed yields a tilted FoP with borders possibly containing unwanted elements. We must then cast this raw FoP onto a perfect rectangle more representative of the reality. To do this, we use a perspective transformation. We begin by approximating the contour by a four sided polygon using a Douglas-Peucker algorithm, which recursively divide the original contour until the variation between the newly computed contour and the original one is the one specified[[2]](#2). Once we have our 4 vertices, we compute the matrix casting them onto a perfect rectangle of width and height same as the maximum width and height of the polygon. \n",
    "\n",
    "INSERT MATH\n",
    "\n",
    "This transformation is a combination of a similarity, a shear transformation, a scaling and an elation that can be formulated that way [[3]](#3): \n",
    "\n",
    "$$ \n",
    "out(x,y) =\n",
    "\\begin{bmatrix}\n",
    "s \\times cos(\\theta) & -s \\times sin(\\theta) & t_{x} \\\\\n",
    "s \\times sin(\\theta) & s \\times cos(\\theta) & t_{y} \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & k & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 0 & 0 \\\\\n",
    "0 & 1/\\lambda & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "v_{1} & v_{2} & v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These are the code snippets highlighting the main aformentioned points:\n",
    "\n",
    "---\n",
    "```r\n",
    "# approximate the field of play to a polygon with 4 vertices\n",
    "epsilon = 0.05 # 5% delta between contour and approximation\n",
    "perimeter = cv.arcLength(c, True) # True means closed contour\n",
    "approximation = cv.approxPolyDP(c, epsilon * perimeter, True)\n",
    "\n",
    "# do the perspective transformation\n",
    "matrix = cv.getPerspectiveTransform(original_coordinates, new_coordinates)\n",
    "fop = cv.warpPerspective(frame, matrix, (int(new_coordinates[3][0]), int(new_coordinates[3][1])))\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the robot starting position\n",
    "\n",
    "It is quite easy to get the location of thymio with our set up. We simply look at the white zone on the FoP. To do this, we grayscale it to get the average BGR pixel intensity and threshholds on a high value. What is more complicated is getting the starting angle of the robot. This is done by looking for the curved part of the robot. We make two different polygonal approximations of the robot, both precise but with slightly different precisions. We then use a bitwise XOR to get the slight difference in curvature and compute the center of this difference. Once we have it, we compute the vector from the center of the robot to the center of the difference and get the complex argument of this vector, interpreting the y coordinate as the imaginary part. The argument is computed that way[[4]](#4):\n",
    "\n",
    "$$\n",
    "Arg(x,y)=\n",
    "\\begin{cases}\n",
    "arctan(\\dfrac{y}{x}) & \\quad \\text{when $x > 0$}\\\\ \n",
    "arctan(\\dfrac{y}{x}) + \\pi & \\quad \\text{when $x < 0 \\: \\& \\: y \\geq 0$}\\\\ \n",
    "arctan(\\dfrac{y}{x}) - \\pi & \\quad \\text{when $x < 0 \\: \\& \\: y < 0$}\\\\ \n",
    "\\dfrac{\\pi}{2} & \\quad \\text{when $x = 0 \\: \\& \\: y > 0$}\\\\ \n",
    "-\\dfrac{\\pi}{2} & \\quad \\text{when $x = 0 \\: \\& \\: y < 0$}\\\\ \n",
    "undefined & \\quad \\text{when $x = 0 \\: \\& \\: y = 0$}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the obstacles\n",
    "\n",
    "The contours of the obstacles are obtained in a similar way than the FoP and the starting position of the robot, the math behind it has already been explained in said sections. What has not yet been explained is how we get the vertices placed futher away from the obstacles to avoid the robot crashing in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[1]: <a id=\"1\">Wikipedia, “Extended Kalman filter” [Last access 25 November 2024]. Available: https://en.wikipedia.org/wiki/Extended_Kalman_filter</a>\n",
    "\n",
    "[2]: <a id=\"2\">Wikipedia, “Ramer–Douglas–Peucker algorithm” [Last access 25 November 2024]. Available: https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm</a>\n",
    "\n",
    "[3]: <a id=\"3\">Medium, “Part II: Projective Transformations in 2D” [Last access 26 November 2024]. Available: https://medium.com/@unifyai/part-ii-projective-transformations-in-2d-2e99ac9c7e9f</a>\n",
    "\n",
    "[4]: <a id=\"4\">Wikipedia, “Argument (complex analysis)” [Last access 25 November 2024]. Available: https://en.wikipedia.org/wiki/Argument_(complex_analysis)</a>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
